{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c32382f-c13c-4f83-bb3a-77d531f0da83",
   "metadata": {},
   "source": [
    "## Proof of concept exploration of null mechanism for SPAWN\n",
    "\n",
    "Goal: Demonstrate how a null mechanism could in principle result in cross structural priming. \n",
    "\n",
    "#### Primes\n",
    "\n",
    "* SC Prime: The teacher thought that/NULL-THAT1 the student appreciated the idea\n",
    "* RC Prime: The teacher appreciated the idea that/NULL-THAT2 the student expressed\n",
    "\n",
    "#### Target\n",
    "The professor announced ____ the exam will be next week.\n",
    "\n",
    "Measure: ____ filled with that or NULL-THAT1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3ba5a0c7-0647-438c-90eb-7e44cbd77033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-16.11809565095832"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import math\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import pickle\n",
    "import ccg\n",
    "import math\n",
    "\n",
    "math.log(eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b816a1-cb39-4a85-b6a5-1a9614d45b5d",
   "metadata": {},
   "source": [
    "## Representational assumptions\n",
    "\n",
    "**Assumption 1** We store overt categories and null categories as separate syntax chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "de533f25-5f50-4fd9-a3ed-042c91bdeae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "overt_chunks = {\n",
    "    'Det': {\n",
    "        'left': 'NP',\n",
    "        'right': 'N',\n",
    "        'combinator': '/'\n",
    "    },\n",
    "    'Noun': {\n",
    "        'left': 'N',\n",
    "        'right': '',\n",
    "        'combinator': ''\n",
    "    },\n",
    "    'SC_verb':{\n",
    "        'left': '(S\\\\NP)',\n",
    "        'right': 'S',\n",
    "        'combinator': ''\n",
    "    },\n",
    "    'Tr_verb':{\n",
    "        'left': '(S\\\\NP)',\n",
    "        'right': 'NP',\n",
    "        'combinator': '/'\n",
    "    },\n",
    "    'that1':{\n",
    "        'left': 'S',\n",
    "        'right': 'S',\n",
    "        'combinator': '/'\n",
    "    },\n",
    "    'that2':{\n",
    "        'left': '(N/N)',\n",
    "        'right': '(S/NP)',\n",
    "        'combinator': '/'\n",
    "    }\n",
    "\n",
    "}\n",
    "\n",
    "## these don't have to be structures in syntax_chunks. Can be anything. \n",
    "null_chunks = {\n",
    "    'NULL-THAT1': syntax_chunks['that1'],\n",
    "    'NULL-THAT2': syntax_chunks['that2']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5504fa5c-0659-431d-9347-a7583f1de6df",
   "metadata": {},
   "source": [
    "**Assumption 2**: Every syntax chunk (overt and null) is associated with a base-level activation which is influenced by how frequently and recently the chunk was retrieved. (This assumption is the same as in the og SPAWN model) \n",
    "\n",
    "**Assumption 3**: Each overt chunk has spreading activation to null chunks which indicates how likely is the overt chunk to be followed by a null chunk. (This is like lexical activation, but instead of being tied to specific words, it is broader and is tied to categories). \n",
    "\n",
    "## Set up toy activations\n",
    "\n",
    "* Currently just based on frequency, without any decay. \n",
    "* Exposed to 4 sentences: one for each prime type and overt vs. covert THAT.\n",
    "* Det and Noun occur in all sentences -- so their value is 4\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5824f0bc-e816-48d1-80ac-c37828af7da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NULL-THAT1': 1, 'NULL-THAT2': 1, 'not-null': 14}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## These numbers are not looking at entire sentence just the first part. \n",
    "null_count_bytag = {\n",
    "    'Det': {\n",
    "        'NULL-THAT1': 0,\n",
    "        'NULL-THAT2': 0,\n",
    "        'not-null': 4\n",
    "    },\n",
    "\n",
    "    'Noun': {\n",
    "        'NULL-THAT1': 0,\n",
    "        'NULL-THAT2': 0,\n",
    "        'not-null': 4\n",
    "    },\n",
    "\n",
    "    'SC_verb': {\n",
    "        'NULL-THAT1': 1,\n",
    "        'NULL-THAT2': 0,\n",
    "        'not-null': 1\n",
    "    },\n",
    "    'Tr_verb': {\n",
    "        'NULL-THAT1': 0,\n",
    "        'NULL-THAT2': 1,\n",
    "        'not-null': 1\n",
    "    },\n",
    "\n",
    "    'that1': {\n",
    "        'NULL-THAT1': 0,\n",
    "        'NULL-THAT2': 0,\n",
    "        'not-null': 2\n",
    "    },\n",
    "\n",
    "    'that2': {\n",
    "        'NULL-THAT1': 0,\n",
    "        'NULL-THAT2': 0,\n",
    "        'not-null': 2\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "null_count = {\n",
    "        'NULL-THAT1': 0,\n",
    "        'NULL-THAT2': 0,\n",
    "        'not-null': 0 \n",
    "    }\n",
    "\n",
    "for null in null_count: \n",
    "    for overt in null_count_bytag:\n",
    "        null_count[null] += null_count_bytag[overt][null]\n",
    "\n",
    "\n",
    "print(null_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef75194-074c-4fbe-95c9-74aa2857b9d7",
   "metadata": {},
   "source": [
    "## Specify null mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d5d75975-9132-453c-ba48-b3364bf5d46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_null(word, curr_tag, parse_state, null_base, null_base_bytag, null_chunks, tr_rules, noise_sd):\n",
    "\n",
    "    found_valid = False\n",
    "    poss_tags = list(null_base.keys())\n",
    "\n",
    "    while not found_valid: \n",
    "        ## initialize with base-level count\n",
    "        act_dict = {tag: null_base[tag] for tag in poss_tags}\n",
    "\n",
    "        ## add in count from current tag\n",
    "        for tag in act_dict:\n",
    "            act_dict[tag] += null_base_bytag[curr_tag][tag]\n",
    "\n",
    "        ## log transform\n",
    "        eps = 0.0001\n",
    "        for tag in act_dict:\n",
    "            act_dict[tag] = math.log(act_dict[tag] + eps)\n",
    "        \n",
    "        \n",
    "        ## add in noise \n",
    "        for tag in act_dict:  \n",
    "            act_dict[tag] += np.random.normal(0, noise_sd)\n",
    "\n",
    "        null_pred = max(act_dict, key=lambda key:act_dict[key])\n",
    "\n",
    "        if null_pred == 'not-null':\n",
    "            found_valid = True\n",
    "        else:\n",
    "            null_chunk = null_chunks[null_pred]\n",
    "\n",
    "            combined = ccg.combine(\n",
    "                tag = null_chunk,\n",
    "                parse_state = parse_state,\n",
    "                tr_rules = tr_rules)\n",
    "\n",
    "            if combined:\n",
    "                found_valid = True\n",
    "            else: ## invalid tag with parse state\n",
    "                poss_tags.remove(null_pred) \n",
    "    \n",
    "    return null_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05e0fef-5c24-4ced-a93f-a2c0b4422410",
   "metadata": {},
   "source": [
    "Other things to implement/ think through: \n",
    "\n",
    "* Lexical activation\n",
    "* Reanalysis w.r.t. null elements + inhibition "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e493535-5c1f-4352-8b8d-ca4a10511375",
   "metadata": {},
   "source": [
    "## Basic priming experiment\n",
    "\n",
    "**Counts of null-prediction for 1000 runs with no priming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "907de08b-500d-46b1-a06d-00933bb78c79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NULL-THAT1': 67, 'NULL-THAT2': 0, 'not-null': 933}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc_parse_state = {\n",
    "    'left': 'S',\n",
    "    'right': 'S',\n",
    "    'combinator': '/'\n",
    "}\n",
    "\n",
    "counts = {key: 0 for key in null_categories}\n",
    "counts['not-null'] = 0\n",
    "\n",
    "for i in range(1000):\n",
    "    null = predict_null(word = 'announced', \n",
    "                 curr_tag = 'SC_verb',\n",
    "                 parse_state = sc_parse_state,\n",
    "                 null_base = null_count,\n",
    "                 null_base_bytag = null_count_bytag,\n",
    "                 null_chunks = null_chunks,\n",
    "                 tr_rules = {},\n",
    "                 noise_sd = 1)\n",
    "    counts[null]+=1\n",
    "\n",
    "counts          \n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5088b664-67b2-412c-9534-3ba55befffce",
   "metadata": {},
   "source": [
    "**Counts of null-prediction for 1000 runs with one SC prime with null-that**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ac15bb32-a475-4459-bcb3-396159dc7159",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NULL-THAT1': 334, 'NULL-THAT2': 0, 'not-null': 666}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_bytag_scprime = deepcopy(null_count_bytag)\n",
    "null_bytag_scprime['SC_verb']['NULL-THAT1']+=1\n",
    "null_bytag_scprime['SC_verb']['not-null']+=2\n",
    "\n",
    "null_scprime = deepcopy(null_act)\n",
    "null_scprime['NULL-THAT1'] +=1\n",
    "\n",
    "\n",
    "counts = {key: 0 for key in null_categories}\n",
    "counts['not-null'] = 0\n",
    "\n",
    "for i in range(1000):\n",
    "    null = predict_null(word = 'announced', \n",
    "                 curr_tag = 'SC_verb',\n",
    "                 parse_state = sc_parse_state,\n",
    "                 null_base = null_scprime,\n",
    "                 null_base_bytag = null_bytag_scprime,\n",
    "                 null_chunks = null_chunks,\n",
    "                 tr_rules = {},\n",
    "                 noise_sd = 1)\n",
    "    counts[null]+=1\n",
    "\n",
    "counts          \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef78b284-f1ee-4aef-b65e-d8ec7bbfa3ef",
   "metadata": {},
   "source": [
    "**Counts of null-prediction for 1000 runs with one RC prime with null-that**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c3d3c968-1366-43f3-a0e5-024cd1aa7da5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NULL-THAT1': 213, 'NULL-THAT2': 0, 'not-null': 787}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_bytag_rcprime = deepcopy(null_count_bytag)\n",
    "null_bytag_rcprime['Tr_verb']['NULL-THAT2']+=1\n",
    "null_bytag_scprime['Tr_verb']['not-null']+=2\n",
    "\n",
    "null_rcprime = deepcopy(null_act)\n",
    "null_rcprime['NULL-THAT2'] += 1\n",
    "\n",
    "counts = {key: 0 for key in null_categories}\n",
    "counts['not-null'] = 0\n",
    "\n",
    "for i in range(1000):\n",
    "    null = predict_null(word = 'announced', \n",
    "                 curr_tag = 'SC_verb',\n",
    "                 parse_state = sc_parse_state,\n",
    "                 null_base = null_rcprime,\n",
    "                 null_base_bytag = null_bytag_scprime,\n",
    "                 null_chunks = null_categories,\n",
    "                 tr_rules = {},\n",
    "                 noise_sd = 1)\n",
    "    counts[null]+=1\n",
    "\n",
    "counts      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
